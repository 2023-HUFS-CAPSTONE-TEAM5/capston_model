{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjdDRhPWBLLo",
        "outputId": "0b57aa1a-cf3b-4fd2-b6d6-6a428b5d528f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBnHlNo7BquQ",
        "outputId": "fd5096f8-6aa3-4550-a3b6-c0a9d4114b4d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "웹 main에서 추가해야 할 코드\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import torch\n",
        "\n",
        "data = Data(음성 파일, 사용자 성별)\n",
        "PKL_LOCATION = generate_pkl(data.wav_file)\n",
        "test_set = Voice_dataset(pkl_location = PKL_LOCATION)\n",
        "test_loader = DataLoader(test_set, batch_size=len(test_set), shuffle=False, num_workers=8)\n",
        "\n",
        "MALE_PATH =  # male_best_model_epoch_ _.pth 저장 경로 \n",
        "FEMALE_PATH =  # female_best_model_epoch_ _.pth 저장 경로 \n",
        "\n",
        "# 초기 모델 선언 (모델 구조 저장)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNNTransformer(num_emotions=8).to(device)\n",
        "\n",
        "# Test\n",
        "gender = data.gender\n",
        "if gender == \"male\":\n",
        "    emotions_ratio, max_emotion = test(model, test_loader, path=MALE_PATH)\n",
        "elif gender == \"female\":\n",
        "    emotions_ratio, max_emotion = test(model, test_loader, path=FEMALE_PATH)\n",
        "\"\"\"\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import sys\n",
        "# ignore warnings\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import os\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from collections import Counter\n",
        "from pydub import AudioSegment\n",
        "import soundfile\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "class Data:\n",
        "    def __init__(self, wav_file, gender):\n",
        "        self.wav_file = wav_file\n",
        "        self.gender = gender\n",
        "\n",
        "\n",
        "# Data Pre-processing\n",
        "def MELSpectrogram(signal, sample_rate):\n",
        "    mel_spec = librosa.feature.melspectrogram(y=signal, sr=sample_rate, n_fft=1024, hop_length=256, n_mels=128, fmax=sample_rate / 2)\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "    return mel_spec_db\n",
        "\n",
        "def generate_pkl(INPUT_WAV_PATH): # 입력된 wav 파일을 .pkl(입력 음성의 경로, 멜스펙트로그램 포함) 형식으로 변환\n",
        "    '''Initializations'''\n",
        "    # SAMPLE_RATE = 48000 # 1차 모델용 sr\n",
        "    DURATION = 3.0\n",
        "    SAMPLE_RATE = librosa.get_samplerate(INPUT_WAV_PATH)\n",
        "    audio, _ = librosa.load(INPUT_WAV_PATH, duration=DURATION, offset=10.0, sr=SAMPLE_RATE)\n",
        "\n",
        "    df_path =  pd.DataFrame(columns=[\"path\"])\n",
        "    df_mel = pd.DataFrame(columns=[\"feature\"])\n",
        "\n",
        "    audio, _ = librosa.effects.trim(audio, top_db=60) # 묵음 처리\n",
        "\n",
        "    for i, p in enumerate(INPUT_WAV_PATH):\n",
        "        SAMPLE_RATE = librosa.get_samplerate(INPUT_WAV_PATH)\n",
        "        temp_audio = np.zeros((int(SAMPLE_RATE*DURATION,)))\n",
        "        temp_audio[:len(audio)] = audio\n",
        "        mel = MELSpectrogram(temp_audio, sample_rate=SAMPLE_RATE)\n",
        "        df_path.loc[i] = p\n",
        "        df_mel.loc[i]= [mel]\n",
        "\n",
        "    df = pd.concat([df_path, df_mel], axis=1)\n",
        "    PKL_PATH = \"/content/drive/MyDrive/apple_farm/\"  ########## .pkl(test데이터) 저장할 경로##########\n",
        "    df.to_pickle(PKL_PATH + \"test\")\n",
        "    PKL_LOCATION = os.path.join(PKL_PATH + \"test\")\n",
        "\n",
        "    return PKL_LOCATION\n",
        "\n",
        "\n",
        "# test.pkl을 Pytorch의 Dataset 형태로 변환해주는 함수\n",
        "class Voice_dataset(Dataset):\n",
        "    def __init__(self, pkl_location):\n",
        "        self.df = pd.read_pickle(pkl_location)\n",
        "\n",
        "    def normalize(self, data):\n",
        "        return minmax_scale(data, feature_range=(0, 1))\n",
        "\n",
        "    def __len__(self):  # returns the length of the data set\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        voice = dict()\n",
        "        voice[\"features\"] = self.df.iloc[idx, 1]\n",
        "        return voice\n",
        "\n",
        "\n",
        "# 사용한 모델의 구조 (모델 불러오기 위해 필요)\n",
        "class CNNTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, num_emotions):\n",
        "        super().__init__()\n",
        "        # conv block\n",
        "        self.conv2Dblock = nn.Sequential(\n",
        "\n",
        "            # 1. conv block\n",
        "            nn.Conv2d(  in_channels=1,\n",
        "                        out_channels=16,\n",
        "                        kernel_size=3,\n",
        "                        stride=1,\n",
        "                        padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(p=0.3),\n",
        "            # 2. conv block\n",
        "            nn.Conv2d(  in_channels=16,\n",
        "                        out_channels=32,\n",
        "                        kernel_size=3,\n",
        "                        stride=1,\n",
        "                        padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
        "            nn.Dropout(p=0.3),\n",
        "            # 3. conv block\n",
        "            nn.Conv2d(  in_channels=32,\n",
        "                        out_channels=64,\n",
        "                        kernel_size=3,\n",
        "                        stride=1,\n",
        "                        padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
        "            nn.Dropout(p=0.3),\n",
        "            # 4. conv block\n",
        "            nn.Conv2d(  in_channels=64,\n",
        "                        out_channels=64,\n",
        "                        kernel_size=3,\n",
        "                        stride=1,\n",
        "                        padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
        "            nn.Dropout(p=0.3))\n",
        "\n",
        "        # Transformer block\n",
        "        self.transf_maxpool = nn.MaxPool2d(kernel_size=[2, 4], stride=[2, 4])\n",
        "        transf_layer = nn.TransformerEncoderLayer(  d_model=64,\n",
        "                                                    nhead=4,\n",
        "                                                    dim_feedforward=512,\n",
        "                                                    dropout=0.4,\n",
        "                                                    activation='relu')\n",
        "        self.transf_encoder = nn.TransformerEncoder(transf_layer, num_layers=4)\n",
        "\n",
        "        # Linear softmax layer\n",
        "        self.out_linear = nn.Linear(320, num_emotions)\n",
        "        self.dropout_linear = nn.Dropout(p=0)\n",
        "        self.out_softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # conv embedding\n",
        "        conv_embedding = self.conv2Dblock(x)  #(b,channel,freq,time)\n",
        "        conv_embedding = torch.flatten(\n",
        "            conv_embedding, start_dim=1)  # do not flatten batch dimension\n",
        "\n",
        "        # transformer embedding\n",
        "        x_reduced = self.transf_maxpool(x)\n",
        "        x_reduced = torch.squeeze(x_reduced, 1)\n",
        "        x_reduced = x_reduced.permute(\n",
        "            2, 0, 1)  # requires shape = (time,batch,embedding)\n",
        "        transf_out = self.transf_encoder(x_reduced)\n",
        "        transf_embedding = torch.mean(transf_out, dim=0)\n",
        "\n",
        "        # concatenate\n",
        "        complete_embedding = torch.cat([conv_embedding, transf_embedding], dim=1)\n",
        "\n",
        "        # final Linear\n",
        "        output_logits = self.out_linear(complete_embedding)\n",
        "        output_logits = self.dropout_linear(output_logits)\n",
        "        output_softmax = self.out_softmax(output_logits)\n",
        "        return output_softmax\n",
        "\n",
        "\n",
        "# Test\n",
        "def print_test_result(predictions):\n",
        "\n",
        "    total_count = {\n",
        "        \"neutral\": predictions[0],\n",
        "        \"happy\": predictions[1],\n",
        "        \"sad\": predictions[2],\n",
        "        \"angry\": predictions[3],\n",
        "        \"fearful\": predictions[4],\n",
        "        \"disgust\": predictions[5],\n",
        "        \"surprised\": predictions[6]\n",
        "    }\n",
        "\n",
        "    emotion_ratio = {}\n",
        "    for emotion in total_count.keys():\n",
        "        # emotion_ratio[emotion] = round((total_count[emotion]) * 100, 2)\n",
        "        print(f\"{emotion} : {total_count[emotion] * 100:.5f}%\")\n",
        "\n",
        "    max_emotion = max(total_count, key=total_count.get)\n",
        "    print(f'가장 큰 비율을 차지하고 있는 감정은 \"{max_emotion}\" 입니다.')\n",
        "\n",
        "    return emotion_ratio, max_emotion\n",
        "\n",
        "# helper function for computing model accuracy\n",
        "def test(model, loader, path=None):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # 성별에 따라 다르게 학습된 모델 load => 초기 모델에 학습된 모델의 가중치 덮어씌우기\n",
        "    model.load_state_dict(torch.load(path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_preds_emotions = list()\n",
        "        for data in loader:\n",
        "            features = data[\"features\"].unsqueeze(1).float().to(device)\n",
        "            predictions = model(features)\n",
        "            # print(features)\n",
        "        predictions = predictions[0].tolist()\n",
        "        # for i in range (len(predictions)):\n",
        "        #     print((predictions[i]*100))\n",
        "    return print_test_result(predictions)"
      ],
      "metadata": {
        "id": "LqmKUTgYCLLt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Data(\"/content/drive/MyDrive/apple_farm/test.wav\", \"female\")\n",
        "PKL_LOCATION = generate_pkl(data.wav_file)\n",
        "test_set = Voice_dataset(pkl_location = PKL_LOCATION)\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "MALE_PATH = \"/content/drive/MyDrive/apple_farm/checkpoints/weights/2nd_model/Male/2nd_best_model_epoch_55.pth\" # male_best_model_epoch_ _.pth 저장 경로 \n",
        "FEMALE_PATH = \"/content/drive/MyDrive/apple_farm/checkpoints/weights/CNN_Transformer/Female/best_model_epoch_110.pth\" # female_best_model_epoch_ _.pth 저장 경로 \n",
        "\n",
        "# 초기 모델 선언 (모델 구조 저장)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNNTransformer(num_emotions=8).to(device)\n",
        "\n",
        "if data.gender == \"male\":\n",
        "    # test(model, test_loader, path=MALE_PATH)\n",
        "    emotions_ratio, max_emotion = test(model, test_loader, path=MALE_PATH)\n",
        "elif data.gender == \"female\":\n",
        "    # test(model, test_loader, path=FEMALE_PATH)\n",
        "    emotions_ratio, max_emotion = test(model, test_loader, path=FEMALE_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TA_Kap2Uotn",
        "outputId": "8f7e69cb-2606-4fd5-8d74-045e40ef8e4f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neutral : 32.40249%\n",
            "happy : 0.45197%\n",
            "sad : 2.54159%\n",
            "angry : 2.42978%\n",
            "fearful : 0.73520%\n",
            "disgust : 0.00126%\n",
            "surprised : 61.39070%\n",
            "가장 큰 비율을 차지하고 있는 감정은 \"surprised\" 입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qo28A0W2DujR"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}